---
title: "Mixture Hüsler--Reiss"
output: 
  pdf_document:
    extra_dependencies: ["bbm", "threeparttable","algorithm","algpseudocode"]
date: '\today'
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
- \usepackage{bbm}
- \usepackage{H}
---







```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newcommand{\GP}{\operatorname{GP}}  
\newcommand{\Gumbel}{\operatorname{Gumbel}}  
\newcommand{\supp}{\operatorname{supp}} 
\newcommand{\MEV}{\operatorname{MEV}} 

\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{a.s}}}{=}}


\newcommand{\expec}{\operatorname{E}}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\Mb}{\mathcal{M}_{b}}
\newcommand{\Cb}{\mathcal{C}_{b}}
\newcommand{\Co}{\mathcal{C}_{0}}
\newcommand{\Mo}{\mathcal{M}_{0}}
\newcommand{\normun}{\|\,\cdot\,\|_1}
\newcommand{\norminfty}{\|\,\cdot\,\|_{\infty}}
\newcommand{\norm}{\|\,\cdot\,\|}
\newcommand{\GEV}{\operatorname{GEV}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\indicator}{\operatorname{\mathbbm{1}}}
\newcommand{\DA}{\operatorname{DA}} 
\newcommand{\ninf}[1]{\left\|{#1}\right\|_{\infty}}
\newcommand{\cone}{\mathcal{C}}
\newcommand{\point}{\,\cdot\,}
\newcommand{\UIj}{\boldsymbol{U}_{\mid I_j}}
\newcommand{\TIj}{\boldsymbol{T}_{\mid I_j}}
\newcommand{\Fr}{\operatorname{Fr}}   
\newcommand{\tg}{\operatorname{tg}} 
\newcommand{\stdf}{\operatorname{stdf}}
\newcommand{\card}{\operatorname{Card}}


\newcommand{\RR}{\mathbb{R}} 
\newcommand{\bo}[1]{\boldsymbol{#1}} 
\newcommand{\diff}{\, \textnormal{d}} 
\newcommand{\eqd}{\overset{\textnormal{d}}{=}} 
\newcommand{\dto}{\overset{\textnormal{d}}{\rightarrow}} 

\newcommand{\vX}{\vc{X}} 
\newcommand{\vx}{\vc{x}}
\newcommand{\eps}{\epsilon}
\newcommand{\borel}{\mathcal{B}}
\newcommand{\sphere}{\mathbb{S}} 



\newcommand{\Ea}{\mathbb{E}_I}
\newcommand{\Eb}{\mathbb{E}_{I,r,\|\, \cdot\,\|}}
\newcommand{\Ec}{ \mathbb{S}_{\|\, \cdot\,\|}}
\newcommand{\Ed}{\mathbb{S}_{I, \|\, \cdot\,\|}}
\newcommand{\itemQ}{%
    \addtocounter{Qx}{1}
    \item[Q\theQx.]}
\newcommand{\am}[1]{\textcolor{red}{\small\sffamily [#1]}}

\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\binfty}{\boldsymbol{\infty}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\HR}{Hüsler--Reiss}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\Rd}{\reals^d}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\phinorm}{\phi_{\|\,\cdot\,\|}}
\newcommand{\simplex}{\mathbb{S}_{\|\,\cdot\,\|}}

\definecolor{navyblue}{rgb}{0.0, 0.0, 0.5}
\definecolor{darkcerulean}{rgb}{0.03, 0.27, 0.49}
\definecolor{burntorange}{rgb}{0.8, 0.33, 0.0}
\definecolor{britishracinggreen}{rgb}{0.0, 0.26, 0.15}
\definecolor{alizarin}{rgb}{0.82, 0.1, 0.26}
\newcommand{\js}[1]{\textcolor{navyblue}{\sffamily\footnotesize [#1]}}
\newcommand{\ak}[1]{\textcolor{olive}{\sffamily\footnotesize [#1]}}
\newcommand{\ltx}[1]{\textcolor{darkcerulean}{\sffamily\footnotesize [#1]}}
\newcommand{\wrt}[1]{\textcolor{britishracinggreen}{\sffamily\footnotesize [#1]}}
\newcommand{\cnt}[1]{\textcolor{burntorange}{\sffamily\footnotesize [#1]}}
\newcommand{\red}[1]{\textcolor{alizarin}{\sffamily\footnotesize [#1]}}
\newcommand{\todo}[1]{\textcolor{burntorange}{\sffamily\footnotesize [TODO: #1]}}

\newcommand{\Normal}{\mathcal{N}}
\newcommand{\matrice}{\mathcal{M}}


\newcommand{\pr}{\Pr}
















Let $A=(a_{ij})_{i\leq d, j \leq k} \in \mathcal{M}_{d,k}([0,1])$ be a matrix such that $\sum_{j=1}^{k} a_{ij}=1$ for all $j\in \{1,\ldots,k\}$. For $j \in \{1,\ldots,k\}$, let $\boldsymbol{U}^{(j)} \sim  \mathcal{N}(-(1/2)\diag(\Sigma^{(j)}), \Sigma^{(j)})$ where $\Sigma^{(j)}\in \mathbb{M}_{d,d}(\mathbb{R})$ is a positive semidefinite matrix. Let $\boldsymbol{Z}^{(1)},\ldots,\boldsymbol{Z}^{(k)}$ be independent random vectors that follow multivariate evd's with unit Fréchet margins and a dependence structure generated by $\boldsymbol{U}^{(1)},\ldots,\boldsymbol{U}^{(k)}$, respectively. Consider 
$$
\begin{aligned}
\boldsymbol{M}=\left(\max_{j=1}^{k} a_{1j}Z_{1}^{(j)}, \ldots,\max_{j=1}^{k} a_{dj}Z_{d}^{(j)}\right).
\end{aligned}
$$
The random vector $\boldsymbol{M}$ follows a multivariate evd which we will denote $G_M$. Consider $H=\GP(G_M)$ the mgpd associated to $G_M$ after changing the location parameter so that $G_M(\bo{0})>0$. Let $\ell$ denote the stdf associated to the latter function. We have proved that the random vector $\bo{U}_{\text{Mix},A}$ defined in $[-\binfty,\binfty)$ by 
    \begin{equation}
         \pr(\bo{U}_{\text{Mix},A} \in B)= \sum_{j=1}^{k} \pi_j \pr\left(\bU^{(j)}+\ln(\bo{a}_{\point j}/ \pi_j) \in B\right), \qquad B \in \mathcal{B}([-\binfty,\binfty))
         \label{equationgeneralgenerator}
    \end{equation}
is a generator of $\ell$. To simplify notation, write $\bo{U}_{\text{Mix},A}=\bU$. Let $\bT$ be a $\bT$-representation of $H$. For $j=1,\ldots,k$, write $I_{j}=\{i \in \{1,\ldots,d\}: a_{ij}>0\}$ the signature of the $j$-th column from $A$.
In the following, for $\bX$ a $d$-variate random vector with lower endpoints $\kappa_1,\ldots,\kappa_d$ and  $\emptyset \neq I \subset D$ such that $\pr[X_i>\kappa_i \text{ iff } \in I]>0$, let $\bX_{\mid I}$ denote a $\lvert I \rvert$-variate random vector on $\prod_{i \in I} (\kappa_i,\infty)$ such that $\mathcal{L}(\bX_{\mid I})= \mathcal{L}(\bX_{I} \mid X_i>\kappa_i \text{ iff } i \in I)$.
\section{ Determine $\mathcal{L}(\bU_{I_{j}})$ }
\label{sec:LawofU} 
Let $j_0 \in \{1,\ldots,k\}$. For $B \in \mathcal{B}( (-\infty,\infty)^{\lvert I_{j_0} \rvert} )$, we have 

$$
\begin{aligned}
\pr(\bU_{\mid I_{j_0}}  \in B)
&:= \pr(\bU_{I_{j_0}} \in B \mid U_i>-\infty \text{ iff } i \in I_{j_0}  )\\
&=\frac{ \pr(\{\bU_{I_{j_0}} \in B\} \cap \{U_i>-\infty \text{ iff } i \in I_{j_0}\}    )  }{  \pr(U_i>-\infty \text{ iff } i \in I_{j_0}) }\\
&=\frac{ \sum_{j=1}^k \tfrac{1}{k} \pr\left( \{ (U_{i}^{(j)} +\ln(ka_{i,j}) )_{i \in I_j} \in B \} \cap \{U_{i}^{(j)}+\ln(k a_{i,j})>-\infty \text{ iff } i \in I_{j_0}\}  \right) }{ \frac{1}{k}   }\\
&=\frac{ \tfrac{1}{k} \pr( (U_{i}^{(j_0)} +\ln ka_{i,j_0})_{i \in I_{j_0}} \in B ) }{ \frac{1}{k}   }\\
&= \pr( (U_{i}^{(j_0)} +\ln ka_{i,j_0})_{i \in I_{j_0}} \in B ).
\end{aligned}
$$
This proves that 
\begin{equation}
\mathcal{L}(\boldsymbol{U}_{\mid I_{j_0}}) \sim \mathcal{N}\left\{\left(-(1/2) \Sigma_{i,i}^{(j_0)} +\ln k  a_{i,j_0}\right)_{i \in I_{j_0}}, \Sigma^{(j_0)}_{I_{j_0}} \right\},
\label{equa:lawofU}
\end{equation}


where $\Sigma^{(j_0)}_{I_{j_0}}$ drops the irrelevant rows from $\Sigma^{(j_0)}$.
\section{Calculus of $\mathbb{P}[T_i>-\infty \text{ iff } i \in I_j], \; j=1,\ldots,k$}


Let $j_0 \in \{1,\ldots,k\}$. We have 
$$
\begin{aligned}
\pr[T_i>-\infty \text{ iff } i \in I_{j_0} ]
&=\frac{\expec[e^{\max (\bU)} \indicator \{U_i>-\infty \text{ iff } i \in I_{j_0}\} ]}{\expec[e^{\max (\bU)} ]}\\
&=\frac{\sum_{j=1}^{k} \tfrac{1}{k} \expec\left[ e^{\max(U_{i}^{(j)} +\ln (a_{i,j}k), \; i=1,\ldots,d )} \indicator\{U_{i}^{(j)}+\ln (a_{i,j}k) >-\infty \text{ iff }i \in I_{j_0}    \}  \right]}{\sum_{j=1}^{k} \tfrac{1}{k} \expec\left[ e^{\max(U_{i}^{(j)} +\ln (a_{i,j}k), \; i=1,\ldots,d )}   \right]   }\\
&=\frac{ \expec[e^{\max(U_{i}^{(j_0)} +\ln(a_{i,j_0} k), \; i \in I_{j_0} ) }] }{ \sum_{j=1}^{k} \expec[e^{\max(U_{i}^{(j)} +\ln(a_{i,j} k), \; i \in I_{j} ) }] }\\
&=\frac{\expec[e^{\max \bU_{\mid I_{j_0}}}]}{ \sum_{j=1}^k \expec[e^{\max \bU_{\mid I_{j}}}] },
\end{aligned}
$$

Equation~\eqref{equa:lawofU} yields 

\begin{equation}
\pr[T_i>-\infty \text{ iff } i \in I_{j_0} ]=\frac{\expec[e^{\max \bU_{\mid I_{j_0}}}]}{ \sum_{j=1}^k \expec[e^{\max \bU_{\mid I_{j}}}] }. \label{equa:T}
\end{equation}

\section{Calculus of $\expec[e^{\max \bU_{\mid I_j}}], \; j=1,\ldots,k$}
Let $j \in \{1,\ldots,k\}$ . Let $\Tilde{\Gamma}$ be the matrix in $\mathbb{M}_{d,d}(\mathbb{R})$ whose $(s,t)$-th entry $\Tilde{\gamma}_{s,t}$ is $(\Sigma^{(j)}_{s,s}/2)+ (\Sigma^{(j)}_{t,t}/2)-\Sigma^{(j)}_{s,t}$ if $s\neq t$ and $\Sigma^{(j)}_{s,t}/2$ if $s=t$.
We get 
$$
\begin{aligned}
\mathbb{E}\left[e^{\max \boldsymbol{U}_{\mid I_j} }\right]
&=\mathbb{E}\left[\max\left( \frac{e^{\epsilon_{i} -\Tilde{\gamma}_{ii}}}{z_i }, \; i \in I_{j}\right)\right],
\end{aligned}
$$
where $\epsilon \sim \mathcal{N}(\boldsymbol{0}, \Sigma_{I_j}^{(j)} )$, and $z_i=(a_{ij}k)^{-1}, \; i \in I_j$. If $I_j= \{i\}$ (that is the $j$-th column has only one non-zero row), then this is equal to $ka_{i j}$. If $I_j=\{i_1,i_2\}$, then this is equal to 

$$
ka_{i_1j}\Phi\left(\frac{(2\Tilde{\gamma}_{12})^{1/2}}{2} - \frac{1}{(2\Tilde{\gamma}_{12})^{1/2}} \log \frac{a_{i_2j}}{a_{i_1j}}\right)+
ka_{i_2j}\Phi\left(\frac{(2\Tilde{\gamma}_{12})^{1/2}}{2} - \frac{1}{(2\Tilde{\gamma}_{12})^{1/2}} \log \frac{a_{i_1j}}{a_{i_2j}}\right).
$$

Finally if $\lvert I_j \rvert >2$ then this is equal to
$$
\begin{aligned}
\sum_{i \in I_j}\frac{1}{z_i} \Phi_{p_j-1}(\eta_i, R_i),
\end{aligned}
$$
where $p_{j}= \lvert I_j \rvert$ and $\bo{\eta}_{i}$ is the $(p_{j}-1)$-dimensional vetor with $s$-th component $(\Tilde{\gamma}_{i,s}/2)^{1/2}-\log(z_s/z_i)/(2\Tilde{\gamma}_{i,s})^{1/2} \; (s \neq i)$. The Function $\Phi_{p_j}(\cdot, R)$ denotes the cdf of the $p_j$-variate normal distribution function with mean $\bo{0}$, unit variance and correlation matrix $R$, and finally $R_i$ is the $(p_j-1) \times (p_j-1)$ correlation matrix with $(s,t)$-th entry $(\Tilde{\gamma}_{i,s}+\Tilde{\gamma}_{i,t}-\Tilde{\gamma}_{s,t})/\{2(\Tilde{\gamma}_{i,s}\Tilde{\gamma}_{i,t})^{1/2}\}\; (s,t \neq i)$.

\section{To simulate}



Suppose that $\bo{T}_{\mid I_j}$ is absolutely continuous  w.r.t.\ $\lambda_{p_j}$  with density $f_{\bo{T}_{\mid I_j}}$ for all $j \in \{1,\ldots,k\}$. The random vector $\bo{T}$ is then also absolutely continuous w.r.t.\ a measure $\upsilon$ (See Article 2 for more details). The associated density is then given by 
$$
f_{\bo{T}}(\bo{t})= \sum_{j=1}^{k} \pr[T_i>-\infty \text{ iff } i \in I_j] \cdot f_{\bo{T} _{\mid I_j}}((t_i)_{i \in I_j}).
$$
Let $j \in \{1,\ldots,k\}$. From the one hand, using Expression~\eqref{equa:T} of $\pr[T_i>-\infty \text{ iff } i\in I_j]$  we have 
$$
\begin{aligned}
\frac{\pr[U_i>-\infty \text{ iff } i \in I_j]}{ \pr[T_i>-\infty \text { iff } i \in I_j] \expec[e^{\max \bU}] }
&=\frac{1/k}{ \tfrac{\expec[e^{\max \bU_{\mid I_{j}}}]}{\sum_{l=1}^k \expec[e^{\max \bU_{\mid I_{l}}}] } \expec[e^{\max \bU}]  }\\
&=\frac{1}{ \tfrac{\expec[e^{\max \bU_{\mid I_{j}}}]}{\sum_{l=1}^k \tfrac{1}{k} \expec[e^{\max \bU_{\mid I_{l}}}] } \expec[e^{\max \bU}]  }\\
&=\frac{1}{ \tfrac{\expec[e^{\max \bU_{\mid I_{j}}}]}{\expec[e^{\max \bU}]} \expec[e^{\max \bU}]  }\\
&=\frac{1}{\expec[e^{\max \bU_{\mid I_{j}}}]}.
\end{aligned}
$$

On the other hand, we know that $\bo{U}_{\mid I_j} \sim \Normal \left\{(-\tfrac{1}{2} \Sigma^{(j)}_{i,i} +\ln k a_{i,j})_{i \in I_j}, \Sigma^{(j)}_{I_j}\right\}$, thus for $i \in I_j$ we have $\int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_i} f_{\boldsymbol{U}_{\mid I_j} } (\boldsymbol{s})\diff\boldsymbol{s}=\expec[e^{(\bo{U}_{\mid I_j})_i}]=ka_{i,j}$. Hence we get 
$$
C=\tfrac{\pr[U_i>-\infty \text{ iff } i \in I_j] \sum_{i \in I_j}  \int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_i} f_{\boldsymbol{U}_{\mid I_j}} (\boldsymbol{s})\diff\boldsymbol{s} }{\pr[T_i>-\infty \text{ iff } i \in I_j] \expec[e^{\max (\bo{U})}] }
=\tfrac{\sum_{i \in I_j}  \int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_i} f_{\boldsymbol{U}_{\mid I_j}} (\boldsymbol{s})\diff\boldsymbol{s}}{\expec[e^{\max U_{\mid I_j}}]}
=\tfrac{k\sum_{i \in I_j} a_{i,j}}{\expec[e^{\max U_{\mid I_j}}]},
$$
The density $f_{\TIj}$ is then given by 
$$
\begin{aligned}
f_{\TIj}(\bo{t})
&=\tfrac{\pr[U_i>-\infty \text{ iff } i \in I_j] e^{\max( \bo{t})}}{\pr[T_i>-\infty \text{ iff } i \in I_j] \expec[e^{\max (\bo{U})}] } f_{\UIj}(\bo{t})\\
&=  \tfrac{k\sum_{i \in I_j} a_{i,j}}{\expec[e^{\max\UIj}]}   \cdot  \tfrac{e^{\max (\bo{t})}}{ \sum_{i \in I_j} e^{t_i}} \cdot  \tfrac{\sum_{i \in I_j}  e^{t_i} f_{\UIj} (\boldsymbol{t}) }{\sum_{i \in I_j}  \int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_i} f_{\UIj} (\boldsymbol{s})\diff\boldsymbol{s}}.
\end{aligned}
$$
Moreover we have
$$
q(\bt)
=\tfrac{\sum_{i \in I_j}  e^{t_i} f_{\UIj} (\boldsymbol{t}) }{\sum_{i \in I_j}  \int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_i} f_{\UIj} (\boldsymbol{s})\diff\boldsymbol{s}}
=\sum_{i \in I_j} m_i q_{i}(\bt),
$$
where for $i_{0}$ from $I_j$ we have 
$$
m_{i_0}=\tfrac{\int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_{i_0}} f_{\UIj} (\boldsymbol{s}) \diff \boldsymbol{s}}{\sum_{i \in I_j}  \int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_i} f_{\UIj} (\boldsymbol{s}) \diff \boldsymbol{s}}
=\tfrac{a_{i_0,j}}{\sum_{i \in I_j} a_{i,j}},
$$
and $q_{i_0}(\bt)=\tfrac{e^{t_{i_0}} f_{\UIj} (\boldsymbol{t})}{\int_{\boldsymbol{s} \in \mathbb{R}^{p_j}} e^{s_{i_0}} f_{\UIj} (\boldsymbol{s}) \diff \boldsymbol{s}},$
the density of $\Normal \left\{(-\tfrac{1}{2} \Sigma^{(j)}_{i,i} +\ln k a_{i,j}+\Sigma^{(j)}_{i,i_0})_{i \in I_j}, \Sigma^{(j)}\right\}$. 

We want to simulate from $f_{\TIj}$ and we do so by simulating first $\bo{Q}$ from $q$ and a uniform number $U_0$ on $[0,1]$. We need a uniform upper bound. We have 
$$
\sup_{\bo{t} \in \mathbb{R}^d } \tfrac{f_{\TIj}(t)}{q(\bo{t})}=C \sup_{\bo{t} \in \mathbb{R}^d} \tfrac{e^{\max (\bo{t})}}{\sum_{i=1}^d e^{t_i}}=C=\tfrac{k\sum_{i \in I_j} a_{i,j}}{\expec[e^{\max \UIj}]}
$$
Remark that this is smaller than $kd/\expec[e^{\max \UIj}]$. This is more or less the same bound that we have found for the simple \HR scaled this time by number of columns of the model. 

Then we accept $\bo{Q}$ as a simulation from $f_{\TIj}$ as soon as $U_{0}\leq f_{\TIj}(\bo{Q})/(Cq(\bo{Q}))=e^{\max (\bo{Q})}/\sum_{i=1}^d e^{Q_i}:=LDEO(Q)$
\begin{algorithm}[H]
\caption{  Calculus of $\Tilde{\Gamma}$}\label{alg:cap}
\begin{algorithmic}[1]
\Require {$\Sigma$ positive semidefinite symmetric matrix} 
\Function {matrix\_transformation}{$\Sigma$}
\State $\Tilde{\Gamma}=(\bo{1}\diag(\Sigma)^{t})/2+(\diag(\Sigma)\bone^t)/2-\Sigma$
\State $\diag(\Tilde{\Gamma})=\diag(\Sigma)/2$
\State return($\Tilde{\Gamma}$)
\Comment{This is slightly different from the variogram matrix $\Gamma$, multiplying $\Tilde{\Gamma}$ by $2$ and setting its diagonal to $0$ gives $\Gamma$}
\EndFunction
\end{algorithmic}
\end{algorithm}
```{r}
matrix_transformation <- function(Sigma) {
  gamma <- (as.matrix(rep(1, nrow(Sigma))) %*% diag(Sigma)) / 2 + (as.matrix(diag(Sigma)) %*% rep(1, nrow(Sigma))) / 2 - Sigma
  diag(gamma) <- diag(Sigma) / 2
  return(gamma)
}
```



\begin{algorithm}[H]
\caption{ Calculus of $\bo{\eta_i}$ For $\bo{U} \sim \Normal(\bo{\mu},\Sigma)$ such that $matrix\_transformation(\Sigma)=\Gamma$, and $i \in \{1,\ldots,d\}$}\label{alg:cap}
\begin{algorithmic}[1]
\Require {$\Gamma=(\gamma_{s,t})_{s,t\leq d} \in \matrice_{d,d}(\reals)$, $\bo{\mu} \in \reals^d$, and $i \in \{1,\dots,d\}$} \Comment{$d$ is not a parameter}
\Function {calculus\_of\_eta}{$i,\bo{\mu},\Gamma$}
\State $\bo{\eta_i}=\left((\gamma_{i,s}/2)^{1/2}+\tfrac{(\gamma_{i,i}-\gamma_{s,s}+\mu_i-\mu_s)}{(2\gamma_{i,s})^{1/2}}\right)_{s \neq i}$
\State return($\bo{\eta_i}$)
\EndFunction
\end{algorithmic}
\end{algorithm}

```{r}
calculus_of_eta <- function(i, mu, gamma) {
  eta <- (gamma[i, -i] / 2)^(1 / 2) + (mu[i] + diag(gamma)[i] - mu[-i] - diag(gamma)[-i]) / (2 * gamma[i, -i])^(1 / 2)
  return(eta)
}
```





\begin{algorithm}[H]
\caption{ Calculus of $R_i$ for $i \in \{1,\ldots,d\}$}\label{alg:cap}
\begin{algorithmic}[1]
\Require {$\Gamma=(\gamma_{s,t})_{s,t\leq d} \in \matrice_{d,d}(\reals)$, $\bo{\mu} \in \reals^d$, and $i \in \{1,\dots,d\}$} \Comment{$d$ is not a paramater}
\Function {calculus\_of\_R}{$i,\Gamma$}
\State Define $R \in \matrice_{d-1,d-1}(\reals)$ 
\State $R[s,t]=\tfrac{\gamma_{i,s}+\gamma_{i,t}-\gamma_{s,t}}{\{2(\gamma_{i,s} \gamma_{i,t})^{1/2}\}}, \; s,t \neq i$
\State $\diag(R)=1$
\State return($R$)
\EndFunction
\end{algorithmic}
\end{algorithm}

```{r}
calculus_of_R <- function(i, gamma) {
  R <- (as.matrix(gamma[i, -i]) %*% rep(1, nrow(gamma) - 1) + t(as.matrix(gamma[i, -i]) %*% rep(1, nrow(gamma) - 1)) - gamma[-i, -i]) / (2 * (as.matrix(gamma[i, -i]) %*% rep(1, nrow(gamma) - 1) * t(as.matrix(gamma[i, -i]) %*% rep(1, nrow(gamma) - 1)))^(1 / 2))
  diag(R) <- 1
  return(R)
}
```



\begin{algorithm}[H]
\caption{ Calculus of $\expec[e^{\max(\bo{U})}]$ for $\bo{U}$ a bivariate Gaussian $\Normal(\bo{\mu},\Sigma)$ with Transform\_matrix($\Sigma$)=$\Gamma$}\label{alg:cap}
\begin{algorithmic}[1]
\Require {$\Gamma=(\gamma_{s,t})_{s,t\leq d} \in \matrice_{d,d}(\reals)$, $\bo{\mu} \in \reals^d$}  \Comment{$d$ is not a paramater}
\Function {case\_d\_2}{$\bo{\mu},\Gamma$}
\State $expec=e^{\gamma_{1,1}+\mu_1}\Phi\left( \tfrac{(2\gamma_{12})^{1/2}}{2}-\tfrac{-\mu_1+\mu_2-\gamma_{1,1}+\gamma_{2,2}}{(2 \gamma_{1,2})^{1/2}}\right)+ e^{\gamma_{2,2}+\mu_2}\Phi\left( \tfrac{(2\gamma_{12})^{1/2}}{2}-\tfrac{-\mu_2+\mu_2-\gamma_{2,2}+\gamma_{1,1}}{(2 \gamma_{1,2})^{1/2}}\right) $
\State return(expec)
\EndFunction
\end{algorithmic}
\end{algorithm}

```{r}
case_d_2 <- function(mu, Gamma) {
  expec <- exp(mu[1] + diag(Gamma)[1]) * pnorm(((2 * Gamma[1, 2])^(1 / 2)) / (2) - ((mu[2] - mu[1] + (diag(Gamma)[2] - diag(Gamma)[1])) / ((2 * Gamma[1, 2])^(1 / 2))), mean = 0, sd = 1) + exp(mu[2] + diag(Gamma)[2]) * pnorm(((2 * Gamma[1, 2])^(1 / 2)) / (2) - ((mu[1] - mu[2] + (diag(Gamma)[1] - diag(Gamma)[2])) / ((2 * Gamma[1, 2])^(1 / 2))), mean = 0, sd = 1)
  return(expec)
}
```


\begin{algorithm}[H]
\caption{Calculus of $\expec[e^{\max (\bo{U})}]$ for $\bo{U}$ a $d$-variate $\Normal(\bo{\mu}, \Sigma)$}
\begin{algorithmic}[1]
\Require {$d, \; \Sigma \in \mathcal{M}_{d,d}(\mathbb{R})$ symmetric positive definite matrix, $\bo{\mu} \in \mathbb{R}^d$}
\Function {calculus\_d} {$d,\Sigma,\bo{\mu}$} 
\If{$d=1$} 
    \State return($\exp(\mu+\Sigma/2)$ ) 
\Else 
\State expected\_value=0
\State $\Gamma=matrix\_transformation(\Sigma)$
\If {$d=2$}
 \State return( case\_d\_2($\bo{\mu},\Gamma$)    )
\Else 
\For {$i \in \{1,\ldots,d\}$}
\State $\bo{\eta_i}=calculus\_of\_eta(i,\bo{\mu},\Gamma)$
\State  $R_i=calculus\_of\_R(i,\Gamma)$
\State expected\_value=expected\_value+ $ e^{\mu_i+\gamma_{i,i}} \Phi_{d-1}(\bo{\eta_i},R_i)$, where $\Phi_{d-1}(\point, R_i)$ is the $(d-1)$-Gaussian distribution with mean $\bo{0}$, unit variance and covariance matrix $R_i$ 
\EndFor
\EndIf 

\EndIf
\State return(expected\_value)
\EndFunction
\end{algorithmic}
\end{algorithm}




```{r}
library(MASS)
calculus_d <- function(d, Sigma, mu) {
  if (d == 1) {
    return(exp(mu + (Sigma / 2)))
  } else {
    expectedvalue <- 0
    gamma <- matrix_transformation(Sigma)

    if (d == 2) {
      return(case_d_2(mu, gamma))
    } else {
      for (i in 1:d)
      {
        # Computing eta
        eta <- calculus_of_eta(i, mu, gamma)
        # Computing R
        R <- calculus_of_R(i, gamma)
        expectedvalue <- expectedvalue + exp(mu[i] + gamma[i, i]) * pmvnorm(mean = rep(0, d - 1), corr = R, lower = rep(-Inf, d - 1), upper = eta)
      }
      return(expectedvalue)
    }
  }
}
```



\begin{algorithm}[H]
\caption{Test for the function calculus\_d}
\begin{algorithmic}[1]
\Function {Test} {$n,\Sigma,\bo{\mu}$} 
\Require {$\bo{\mu} \in \Rd$ and $\Sigma \in \mathcal{M}_{d,d}(\reals)$}
\State Simulmate $\bU^{(1)},\ldots,\bU^{(n)}$ from $\Normal(\bo{\mu},\Sigma)$.
\State Calculate $s_i=e^{\max(\bU^{(i)}_{1},\ldots,\bU^{(i)}_{d} )}$ for $i=1,\ldots,d$. \Comment{I think function $\max$ is easier then $\exp$}
\State return( $\sum_{i=1}^{n} s_i/n$ )
\EndFunction
\State Create a matrix 
$$
\begin{aligned}
\Sigma_1=
\begin{pmatrix}
1 & 11/12 & 8/9\\
11/12 & 1 & 10/11 \\
8/9 & 10/11 & 1
\end{pmatrix}
\end{aligned}
$$
\State Create $vector\_test \in \reals^{50}$
\For {i in 1:50}
\State $vector\_test_i=Test(10^6,\Sigma_1,(0,0,0))$
\EndFor
\end{algorithmic}
\end{algorithm}


```{r}
library(MASS)
library(mvtnorm)
Test <- function(n, d, mup, Sigmap) {
  if (d == 1) {
    U <- rnorm(n, mean = mup, sd = Sigmap)
    return(sum(exp(U)) / n)
  } else {
    U <- mvrnorm(n, mu = mup, Sigma = Sigmap)
    f <- function(t) exp(max(t))
    s <- apply(U, 1, f)
    return(sum(s) / n)
  }
}
Sigma_1 <- matrix(0, nrow = 3, ncol = 3)
Sigma_1[1, ] <- c(1, 11 / 12, 8 / 9)
Sigma_1[2, ] <- c(11 / 12, 1, 10 / 11)
Sigma_1[3, ] <- c(8 / 9, 10 / 11, 1)
vector_test <- rep(0, 50)
for (i in 1:50) {
  vector_test[i] <- Test(100000, 3, rep(0, 3), Sigma_1)
}
boxplot(  vector_test,  main = expression(paste("Empirical estimation of our expected value based on sample size ",N=10^5)),    cex.main=0.8)
```

This is the value that we obtained using our algorithm 
```{r}
calculus_d(3, Sigma_1, rep(0, 3))
```

The sample size is $n=10^{5}$, so the standard deviation of our emepirical estimator is smaller then $1/(2\sqrt{10^5})=0.00158$. From the boxplot, this is not satisfied. 




\begin{algorithm}[H]
\caption{ mass\_of\_scenario }
\begin{algorithmic}[1]
\State Define $ mass\_scenario$ 
\Function {function\_mass\_of\_scenario}{$d,k,\Sigma,A$} \Comment{Here $\Sigma$ is a list of matrices}
\Require {$d,\; k,\; A \in \mathbb{M}_{d,d}(\mathbb{R})  $ and a list called $\Sigma$ of $k$ symmetric positive definite matrices of dimension $d$}
\For {$j \in \{1,\ldots,k\}$}
\State d=$\lvert I_j \rvert$
\State $\bo{\mu_j}=(\log(kA_{i,j})-(1/2)\Sigma[[j]]_{i,i})_{i \in I_j}$
\State $R_j=(\Sigma[[j]]_{s,t})_{s \in I_i, t \in I_j}$
\State $\pi_j=calculus\_d(d,R_j,\bo{\mu_j})$
\EndFor
\State $\bo{\pi}=\bo{\pi}/(\sum_{j=1}^{k} \pi_j) $
\State return($\bo{\pi}$)
\EndFunction
\end{algorithmic}
\end{algorithm}

```{r}
library(mvtnorm)

function_mass_of_scenario <- function(d, k, list_of_k_matrices, A) # A is a (dxk) matrix and Sigma is a (dxd) matrix
{
  mass <- rep(0, k)

  # the element pi[j] will contain the mass of the j-th scenario
  for (j in 1:k) {
    dimension <- length(which(A[, j] > 0))
    mu <- log(k * A[which(A[, j] > 0), j]) - (1 / 2) * diag(list_of_k_matrices[[j]])[which(A[, j] > 0)]
    Sigma <- list_of_k_matrices[[j]][which(A[, j] > 0), which(A[, j] > 0)]
    pi[j] <- calculus_d(dimension, Sigma, mu)
  }



  pi <- pi / sum(pi)
  return(pi)
}
```












\begin{algorithm}[H]
\caption{ simulation\_of\_ \HR~mixture }
\begin{algorithmic}[1]
\Function {LDOE}{$\bt \in \Rd$}
\State $\bo{s}=exp(\bt)$
\State return($\max(\bo{s}) / \sum_{i=1}^d e^{s_i}$  )
\EndFunction
\Function {simulation\_of\_mixture}{$d,k,\Sigma,A$} \Comment{Here $\Sigma$ is a list of matrices}
\Require {$d,\; k,\; A \in \mathbb{M}_{d,d}(\mathbb{R})  $ and a list called $\Sigma$ of $k$ symmetric positive definite matrices of dimension $d$}
\State $mass\_of\_scenario=function\_mass\_of\_scenario(d,k,\Sigma,A)$
\State Simulate $U_1$ from $\{1,\ldots,k\}$ such that $\pr(U_1=j)=mass\_of\_scenario[j], \; j \in \{1,\ldots,k\}$
\State cluster=$\{i \in \{1,\ldots,d\}: a_{iU_1}>0\}$
\State $\bo{T}[\{i \in \{1,\ldots,d\}: a_{iU_1}=0\}]=-\infty$
\State $mass\_of\_density=\left(\tfrac{a_{iU_1}}{\sum_{j \in cluser} a_{jU_1}}\right)_{i \in cluster}$
\If {$\lvert cluster \rvert=1$} 
\State $U_2=cluster$
\Else 
\State Simulate $U_2$ from $cluster$ such that $\pr(U_2=i)=mass\_of\_density[i], \; i \in cluster$
\EndIf
\State Simulate $\bo{T}[cluster]$ from $\mathcal{N}\left\{\left(-\tfrac{1}{2} \Sigma^{(U_1)}[i,i]+\Sigma^{(U_1)}[i,U_2]+\ln ka_{iU_1}\right)_{i \in I_{U_1}}, \Sigma^{(U_1)}_{I_{U_1}}  \right\}$ \label{commande1}
\State Simulate $U_0$ from $U([0,1])$  \label{commande2}

\While{$U_0>LDOE(T)$}
    \State repeat \ref{commande1} and \ref{commande2}
\EndWhile  
\State Simulate $E$ a unit exponential random variable independent from $\bo{T}$.
\State $\bo{Y}=\exp(\bo{T}-\max(\bo{T})+E)-1$
\State Return($\bo{Y}$)


\EndFunction
\end{algorithmic}
\end{algorithm}



```{r}
library(mvtnorm)
LDOE <- function(t) {
  s <- exp(t)
  return(max(s) / sum(s))
}
simulation_of_mixture <- function(d, k, Sigma, A) {
  T <- rep(0, d)
  vector_mass_of_scenario <- function_mass_of_scenario(d, k, Sigma, A)
  U1 <- sample(c(1:k), prob = vector_mass_of_scenario, size = 1)
  cluster <- which(A[, U1] > 0)
  T[-cluster] <- -Inf
  mass_of_density <- (A[cluster, U1]) / sum(A[cluster, U1])
  if (length(cluster) == 1) {
    U2 <- cluster
  } else {
    U2 <- sample(cluster, prob = mass_of_density, size = 1)
  }
  T[cluster] <- mvrnorm(1, mu = -(1 / 2) * diag(Sigma[[U1]])[cluster] + log(k * A[cluster, U1]) + Sigma[[U1]][cluster, U2], Sigma = Sigma[[U1]][cluster, cluster])
  U0 <- runif(1, min = 0, max = 1)
  while (U0 > LDOE(T)) {
    T[cluster] <- mvrnorm(1, mu = -(1 / 2) * diag(Sigma[[U1]])[cluster] + log(k * A[cluster, U1]) + Sigma[[U1]][cluster, U2], Sigma = Sigma[[U1]][cluster, cluster])
    U0 <- runif(1, min = 0, max = 1)
  }



  E <- rexp(1, 1)
  Y <- exp(T - max(T) + E) - 1
  return(Y)
}
```














\begin{algorithm}[H]
\caption{ Simulation of $N$ independent mixture \HR~mgp $\bo{Y}$}\label{alg:cap}
\begin{algorithmic}[1]
\Require {$k$-list of $\Sigma$ positive-definite symmetric matrices}
\Function {N\_simulation\_of\_mixture} {$d,k,\Sigma,A,N$} \Comment{Here $\Sigma$ is a list of matrices}
\State N\_simulation=replicate(N, Simulation\_of\_mixture($d,k,\Sigma,A$))
\State return(N\_simulation)
\EndFunction
\end{algorithmic}
\end{algorithm}







```{r}
N_simulation_of_mixture <- function(d, k, list_of_matrices, A, N) {
  N_simulation <- replicate(N, simulation_of_mixture(d, k, list_of_matrices, A))

  return(N_simulation)
}
```


Consider now the covariance matrices 
$$
\begin{aligned}
\Sigma_1=
\begin{pmatrix}
1 & 11/12 & 8/9\\
11/12 & 1 & 10/11 \\
8/9 & 10/11 & 1
\end{pmatrix}, 
\Sigma_2=
\begin{pmatrix}
1 & 1/2 & 1/3\\
1/2 & 1 & 1/4 \\
1/3 & 1/4 & 1
\end{pmatrix},
\Sigma_3=
\begin{pmatrix}
3 & 11/12 & 8/9\\
11/12 & 3 & 10/11 \\
8/9 & 10/11 & 3
\end{pmatrix}.
\end{aligned}
$$
The associated variogram matrices are respectively

$$
\begin{aligned}
\Gamma_1=
\begin{pmatrix}
0 & 0.1666667 & 0.222222\\
0.1666667 & 0 & 0.1818182 \\
0.222222 & 0.1818182 & 0
\end{pmatrix},
 \;\Gamma_2=
\begin{pmatrix}
0 & 1 & 1.33\\
1 & 0 & 1.5 \\
1.5 & 1.33 & 0
\end{pmatrix},
 \;\Gamma_3=
\begin{pmatrix}
0 & 4.166667 & 4.22222\\
4.166667 & 0 & 4.18181818\\
4.22222 &  4.18181818& 0
\end{pmatrix}.
\end{aligned}
$$




```{r}
Sigma <- matrix(0, nrow = 3, ncol = 3)
Sigma[1, ] <- c(1.6, 11 / 12, 8 / 9)
Sigma[2, ] <- c(11 / 12, 1.6, 10 / 11)
Sigma[3, ] <- c(8 / 9, 10 / 11, 1.6)
A <- matrix(0, nrow = 3, ncol = 3)
A[1, ] <- c(1, 0, 0)
A[2, ] <- c(1 / 2, 1 / 2, 0)
A[3, ] <- c(1 / 3, 1 / 3, 1 / 3)
x <- N_simulation_of_mixture(3, 3, list(Sigma, Sigma, Sigma), A, 100)
x1<-x[,which(x[1,]>-1 & x[2,]>-1 & x[3,]>-1)]


# Using as.data.frame

plot(as.data.frame(t(log(x + 1 + 0.0001))), main = expression(paste("Hüsler-Reiss mixture mgp in the exponential scale")), asp = 1)
points(as.data.frame(t(log(x1 + 1 + 0.0001))), col="blue")
```

```{r}
# Ploting on each face
# Face (1,2)
a <- log(x[c(1, 2), ] + 1 + 0.0001)
a <- as.matrix(a)
index <- which(x[1, ] == -1 | x[2, ] == -1)
a1 <- log(x[c(1, 2), index] + 1 + 0.0001)
a1 <- as.matrix(a1)

# Face (1,3)
b <- log(x[c(1, 3), ] + 1 + 0.0001)
b <- as.matrix(b)
index <- which(x[1, ] == -1 | x[3, ] == -1)
b1 <- log(x[c(1, 3), index] + 1 + 0.0001)
b1 <- as.matrix(b1)
# Face (2,3)
c <- log(x[c(2, 3), ] + 1 + 0.0001)
c <- as.matrix(c)
index <- which(x[2, ] == -1 | x[3, ] == -1)
c1 <- log(x[c(2, 3), index] + 1 + 0.0001)
c1 <- as.matrix(c1)

par(mfrow = c(1, 3))
#####
plot(a[2, ] ~ a[1, ], xlab = expression(X[1]), ylab = expression(X[2]), main = "Face (1,2)", asp = 1)
points(a1[2, ] ~ a1[1, ], col = "blue", pch = 19)
#####
plot(b[2, ] ~ b[1, ], xlab = expression(X[1]), ylab = expression(X[3]), main = "Face (1,3)", asp = 1)
points(b1[2, ] ~ b1[1, ], col = "blue", pch = 19)
#####
plot(c[2, ] ~ c[1, ], xlab = expression(X[2]), ylab = expression(X[3]), main = "Face(2,3)", asp = 1)
points(c1[2, ] ~ c1[1, ], col = "blue", pch = 19)
mtext(" 3-dimensional Hüsler-Reiss mixture mgp in the exponential scale", side = 3, line = -1.5, outer = TRUE)

# Using the function pairs
y <- matrix(0, nrow = 100, ncol = 3)
for (i in 1:100) {
  y[i, ] <- log(x[, i] + 1 + 0.0001)
}


pairs(y, main = expression(paste("Hüsler-Reiss mixture mgp in the exponential scale")))


new<-rep(0,100)
new[which(x[1, ] > -1 | x[2, ] > -1 | x[3, ] > -1)]<-1
new[which(x[1, ] == -1 & x[2, ] > -1 & x[3,]>-1)]<-2
new[which(x[1, ] == -1 & x[2, ] == -1 & x[3,]>-1)]<-3


x<-as.data.frame(t(log(x + 1 + 0.0001)))
x$new<-new

#plot(x[x$new==1,c(1,2)], col = "red", xlim=c(-10,5), ylim=c(-10,5))
#points(x[x$new==2,c(1,2)], col = "green")
plot(x[,-4] , xlim=c(-10,5), ylim=c(-10,5),  pch=c(1,2,3)[x$new], col=c("#009999", "#1E7FCB","red")[x$new],cex=1.5)


# index1<-which( (x[1,]==-1) | (x[2,]==-1) )
# points(   log(x[c(1,2),index1]+1+0.001), col = "blue", pch = 19)
# plot(as.data.frame(t(log(x+1+0.0001))), main = expression(paste("Hüsler-Reiss mixture mgp in the exponential scale" )))
extremal_coeficient_111<-function(Sigma){
  return(calculus_d(3, Sigma, -(1/2)*diag(Sigma)+ c(0,log(1/2), log(1/3))) + calculus_d(2, Sigma[2:3,2:3], -(1/2)*diag(Sigma[2:3,2:3])+ c(log(1/2), log(1/3))) +1/3  )
}
extremal_coeficient_111(Sigma)
extremal_coeficient_011<-function(Sigma){
  return(2*calculus_d(2, Sigma[2:3,2:3], -(1/2)*diag(Sigma[2:3,2:3])+ c(log(1/2), log(1/3))) +1/3  )
}
extremal_coeficient_011(Sigma)
```
